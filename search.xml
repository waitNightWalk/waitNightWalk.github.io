<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>神经网络基础</title>
      <link href="/2022/12/14/shen-jing-wang-luo-ji-chu/"/>
      <url>/2022/12/14/shen-jing-wang-luo-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="从输入—-gt-输出的映射"><a href="#从输入—-gt-输出的映射" class="headerlink" title="从输入—&gt;输出的映射"></a>从输入—&gt;输出的映射</h3><h3 id="数学表示"><a href="#数学表示" class="headerlink" title="数学表示"></a>数学表示</h3><p>计算得分值：</p><script type="math/tex; mode=display">f(x,W) = Wx + b</script><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><h3 id="多组权重参数构成了决策边界"><a href="#多组权重参数构成了决策边界" class="headerlink" title="多组权重参数构成了决策边界"></a>多组权重参数构成了决策边界</h3><script type="math/tex; mode=display">f(x_i,W,b) = Wx_i + b</script><p>权重参数 W 控制整个决策边界的走势</p><p>偏置参数 b 只是做微调</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul><li><p>如何衡量分类的结果呢？</p><p>结果的得分值有着明显的差异，我们需要明确的知道模型的当前效果，有多好或是有多差！</p><p>损失函数有很多种，比如:</p></li></ul><script type="math/tex; mode=display">L_{i} = \sum_{j\ne y_i} max(0, s_j + s_{y_i} +1)</script><p>——其中， $s<em>{j}$ 属于其他类别； $s</em>{y_i}$ 属于正确类别； $+1$ 为容忍程度，一般表示为 $\Delta$ 。计算错误类别比正确类别高多少。</p><ul><li>如果损失函数的值相同，那么意味着两个模型一样吗？</li></ul><script type="math/tex; mode=display">f(x,W) = Wx</script><script type="math/tex; mode=display">L=\frac{1}{N} \sum_{i=1}^N \sum_{j \neq y_i} \max \left(0, f\left(x_i, W\right)_j-f\left(x_i, W\right)_{y_i}+1\right)</script><ul><li><p>输入数据： $x=[1,1,1,1]$</p><p>模型A： $w_1=[1,0,0,0]$</p><p>模型B：$w_2=[0.25,0.25,0.25,0.25]$</p><p>$w_1^Tx=w_2^Tx=1$</p><p>模型A只考虑了局部，模型B考虑了全局，均匀分配到整体。要考虑权重参数会不会产生变异或者过拟合。所以在构建损失函数过程中，还要加上一点正则化惩罚项 $\lambda R(W)$ :</p><p>损失函数 = 数据损失(data loss) + 正则化惩罚项</p><p>$\lambda$ 大，消除大噪音、大变异； $\lambda$ 小，稍微消减变异。</p></li></ul><script type="math/tex; mode=display">L=\frac{1}{N} \sum_{i=1}^N \sum_{j \neq y_i} \max \left(0, f\left(x_i, W\right)_j-f\left(x_i, W\right)_{y_i}+1 \right)+ \lambda R(W)</script><ul><li>正则化惩罚项： $R(W) = \sum<em>k\sum_l W</em>{k,l}^2$ （我们总希望模型不要太复杂，过拟合的模型是没用的）</li></ul><h3 id="Softmax-分类器"><a href="#Softmax-分类器" class="headerlink" title="Softmax 分类器"></a>Softmax 分类器</h3><ul><li><p>现在我们得到的是一个输入的得分值，但如果给我一个概率值更好</p><p>如何将一个得分值转换成一个概率值呢？</p><script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}</script><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221215185628864.png" alt="Sigmoid"></p></li><li><p>归一化： $P(Y=k|X=x_i)=\frac{e^sk}{\sum_j e^sj}$ where $s = f(x_i;W)$ </p></li><li><p>计算损失值： $L_i=-logP(Y=k|X=x_i)$ </p><p>输入数据得到得分值，先经过 $e$ 的 $x$ 次幂映射放大，<strong>归一化</strong>得概率后再通过 $L_i$ 进行<strong>损失</strong>计算。</p><p>概率接近1则损失小，为0，概率越小则损失越大。</p></li></ul><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><ul><li><p>得出损失值 data loss：</p><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221215203602356.png" alt=""></p><p>回归任务：由得分值计算损失值；</p><p>分类任务：由概率值计算损失值</p></li><li><p>更新模型方法：反向传播（梯度下降）</p><p>神经网络目标是更新W，使损失降低（优化）</p></li></ul><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>链式法则：梯度是一步一步传播的</p><p>加法门单元：均等分配</p><p>MAX门单元：给最大的</p><p>乘法门单元：互换的感觉</p><hr><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221215221511772.png" alt="神经网络整体架构"></p><ul><li>层次结构</li><li>神经元</li><li>全连接</li><li>非线性</li></ul><h3 id="正则化的作用"><a href="#正则化的作用" class="headerlink" title="正则化的作用"></a>正则化的作用</h3><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221215214116897.png" alt=""></p><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221215214206484.png" alt=""></p><p>神经元越多越容易过拟合</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>常用的激活函数：Sigmoid, Relu, Tanh等</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">//</span>original dataX<span class="token operator">//</span>zero<span class="token operator">-</span>centered dataX <span class="token operator">-=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>X<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">//</span>normalized dataX <span class="token operator">/=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>X<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>通常使用随即策略进行参数初始化</p><p><code>W = 0.01* np.random.randn(D, H)</code></p><h3 id="Drop-Out"><a href="#Drop-Out" class="headerlink" title="Drop-Out"></a>Drop-Out</h3><p>解决过拟合问题（正则化）</p><p>训练时在每一次每一层随机选择一部分不参与传播</p><hr><h2 id="卷积神经网络原理与参数解读"><a href="#卷积神经网络原理与参数解读" class="headerlink" title="卷积神经网络原理与参数解读"></a>卷积神经网络原理与参数解读</h2><p>待续</p>]]></content>
      
      
      <categories>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>“人生太短，普鲁斯特太长”</title>
      <link href="/2022/11/24/ren-sheng-tai-duan-pu-lu-si-te-tai-chang/"/>
      <url>/2022/11/24/ren-sheng-tai-duan-pu-lu-si-te-tai-chang/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/%E6%97%A0%E5%90%8D.jpg"></p><p>20年刚入学时，选出的班长是个文静的女孩。那年的圣诞节她自费给同学们买了糖果，还亲手写了三十几张贺卡送给每个同学，祝大家开心。时至期末考试时，她却缓考许多门，下半学期开学时，连报道都没有来。之后她微信告诉我，自己的抑郁比较严重，就申请休学了。</p><p>今年八月开学前，我去天津探望她，路上她给我发微信：她已经在家无所事事两年了，要是能推荐本书的话，那再好不过。我思来想去，送了一套《追忆似水年华》。无论是从时间还是心境的角度，这部书都更适合病人或闲人，拿来探病也算是不俗的礼物。</p><p>对于大部分读者而言，阅读普鲁斯特都能称之为一种惩罚，意识流的行文风格，过于冗长的内容都足以令人昏昏欲睡。毕竟普鲁斯特本人就谈不上有什么激动的人生经历——他终身缠绵病榻，盖着厚重的鸭绒被，从紧闭的天鹅绒窗帘缝隙窥视世界。哮喘使普鲁斯特的身体饱受折磨，幽居的生活令他的精神更为痛苦，却为世界留下了伟大的作家。</p><p>如果说写作是某种体验到经验的脑力活动，那么普鲁斯特以极逼仄的个人体验，构造出深邃的经验。在自己昏暗的卧室中，他聆听着生命的水滴穿过时空的寂寞之声。透过窗边的苹果树想象卢瓦河边的山楂树与丽春花、枫丹白露的美丽风景。世界是一片汪洋，卧室是他的寂寞方舟。孤独带来精神上的敏锐，他捕捉住从前时空以及当下情景中的一切。从此万象等价，巨细无别，大如星辰，小如沙砾皆可下笔。</p><p>要给如此庞杂的一部小说梳理情节和思想几乎不可能。法国的普氏研究者们把它分为八个主题，分别是时间、人物、社交、爱情、想象、地点、哲学、艺术。在此仅选其中三个主题展开讨论。</p><h1 align = "center"> 爱  情 </h1><p>YOHU网站上的前几篇散文里多有一场失败爱情的影子，或许这可以算的上一个美丽的巧合。《追忆》中的爱情主题也最容易引起读者的共鸣。“爱情”，普鲁斯特说，“是一种相互折磨。”在经历了热情、不安和绝望后，书中任务总不能最终享受自己的情感。“等待”、“失望”和“想象的事物的魔力”是理解普鲁斯特式爱情的关键词。在小说中无论是斯万对奥黛特或是马塞尔对阿尔贝蒂娜，都处在一种两难之中——痛苦于未得之物，或是得到所爱之物后就迷茫于爱的原因。也许爱情的本质是一种痛苦，又也许是这种两难解释了爱情的本质：恋爱对象的不在场才能构成完整的爱情。相比之下还是夏吕斯聪慧，“在生活中重要的不是喜爱什么而是喜爱本身”。</p><h1 align = "center"> 时  间 </h1><p>书名的旨意是“找寻失去的时间”，那么回忆是解开文本密码的另一把钥匙。不妨把回忆分为两种：主动的回忆与被动的回忆。前者借由智力与推理，后者则是完全偶然的。一种气味、一段乐曲将人拉入回忆的长河，而这种无疑是的回忆正是今生今世的证据。水房桌子的冰冷触感会不会把人拉回那个周日的晚自习，某一缕树影间的阳光又会勾起谁的回忆？早上出门时，凛冽的风刮在脸上，寒冷而干燥的空气灌入肺里，那一刻无数个过往冬天的回忆涌上来，仿佛又回到小学时，戴着厚厚手套围巾去上学的哪个早上。林徽因有一句很美的诗：“花有花香，冬有回忆一把”在艺术层面上与普鲁斯特不谋而合。时间的逝去对每个人都同样冷酷，回忆是抵抗时间的唯一方式。在回忆中体验化为经验。存在的质感并不取决于时间的堆积，生命的丰盈其实更在于内在化的体认与理解。所以唯有精神才是构成意义和理解意义的东西。</p><h1 align = "center"> 地  点 </h1><p>沈从文在谈到土豆与慈姑两种食材时评价道：慈姑格高。《追忆》大抵算得上“格高”的小说。普鲁斯特把地点选在了华美的会客厅与优雅的交际场，从而下意识地与平民生活划开了界限。与之相反，中国近现代文学长久地扎根于乡土文化之中，对于寻根有着非凡的热情，作家们以庸俗为美，认定自己摒弃了虚伪与矫饰，从而获得道德上的快意。却也不禁想问：有多少人实在探寻一篇土地的生命记忆，又有多少人是为了名利而在选材上刻意地投机？最令人担心的是：在长久地匍匐在地后，还是否有从世俗尘埃中飞升的能力？在乡土之余，我们可能正需要一份普鲁斯特式的“附庸风雅”，一份“格高”的审美理性。</p><p>在天津把书交给她之后，她问我选择这么厚的一套书作为礼物是否有什么深意。我想了一下，和她说起了第一次与普鲁斯特的相遇。高三九班的北边墙角处写着一行艺术字形的“追忆似水年华”，忆的最后一笔勾下来，与华的最后一笔相连。每每在这个教室模拟考时，我总看着这行小字幻想许久，不知是那一届学生留下来的。他在写这行字时是出于纯粹的无聊，想在压抑的高中生活里添一份色彩，还是有什么别样的含义？毕业后教学楼整体翻新了一次，这行小字应该已经被无情地抹去。但我心底却希望它能幸存，它与六月的夏日格外相配。这些回旋往复的主题如浱暑，蝉鸣，微风，缭绕不去。要在某个霹雳电闪的瞬间，在字里行间看到自己，也看到普鲁斯特所照亮的所有人的痛苦深渊。</p>]]></content>
      
      
      <categories>
          
          <category> 无情岁月有味诗 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>苦旅</title>
      <link href="/2022/11/20/ku-lu/"/>
      <url>/2022/11/20/ku-lu/</url>
      
        <content type="html"><![CDATA[<p>时逢附中百廿周年庆典，见同学所摄校园照片，一些回忆涌上心头。</p><p>朋友说，希望写点积极向上的文章，确实应当如此。然而生活本是苦中作乐，或许更容易写下苦闷的词句。</p><p>其实我并非对于过往的时光有太多的眷恋，只是人的记忆擅长美化，留下些不舍，尤其当我看见空荡荡的教室的模样，突然觉得，在我生命中的某些东西永远地消失了。</p><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/6D85757DAE3A9AF97F0EC2A361256404.png" alt="附中教室"></p><p>我本该可以回母校的，许久前就有所计划，然而因为一些原因，我终究是没去。当我漫步在我的大学校园中，不知不觉走到拉了警戒线的一个教学楼，被门口保安大叔拦住：里面刚考试，据说是大专成人高考，于是一群走路奇怪、身边烟雾缭绕的人好似有了解释。我觉得我见证了部分人的人生。</p><p>思绪回到高一的暑假，怀揣着普通班的分班结果，在回家的公车上，我忆起高一的种种，夹杂着对未来的惶恐，泪水止不住流出。</p><p>我当时对大学的憧憬，至少是自由且不为成绩困扰的60分万岁，事实上我都没有得到。</p><p>我的高中生活，有一段不成熟的感情，朋友喜欢以柏拉图戏称，倒是留下一笔谈资。我想说记不真切了，然而一闭上眼往事仍在我眼前回荡。</p><p>记得曾与大z闲聊时，谈起爱情，他说爱情大概是有一个人可以站在你的旁边，陪你一言不发地望着蓝天发呆。然而事实如卡夫卡所言，“当你站在我的面前看着我时，你知道我心里的悲伤吗？你知道你自己心里的悲伤吗？”人极有可能爱的是自我投射的对方，更难有“挥剑斩情丝”的魄力。</p><p>世界每天发生很多事，我关注了许多，不久便觉无力：我无法改变任何事。这也让我有了重新审视自我价值的契机：人应该为自己活着，忌患得患失，“你能在浪费时间中获得乐趣，就不是浪费时间”。</p><p>许久不读书，我深知自己已经沦陷入自己编织的信息茧房中，然而实在是提不起兴趣，我更喜欢亲眼看看人间百态，或许是人生逆旅的意义。</p><p>深夜所写，确乎难有积极情绪，且相比之下，积极向上的文字似乎更加难以下笔，留给下次罢。</p>]]></content>
      
      
      <categories>
          
          <category> 无情岁月有味诗 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西风多少恨，吹不散眉弯</title>
      <link href="/2022/11/16/xi-feng-duo-shao-hen-chui-bu-san-mei-wan/"/>
      <url>/2022/11/16/xi-feng-duo-shao-hen-chui-bu-san-mei-wan/</url>
      
        <content type="html"><![CDATA[<p>做不了一个人的英雄，就做好另一个人的骑士。但也许男主角本来就是个烂人，没有耐心没有热情，还非要自诩多深情。</p><p>感情真的是个很奇妙的东西，即使身后是不可抵抗的洪流，还是会义无反顾的坠入梦网，去牵手去拥抱。有多少文人墨客，费劲无数笔墨，慷慨高歌爱情，纵使失去，也有着“衣带渐宽终不悔”的觉悟。</p><p>“问世间情为何物 直教人生死相许”。可惜我只有现实主义的躯壳，不足以支撑我爬上雪山之顶采下你这朵高岭之花。很多时候我们之间的吵架都在一遍遍告诉我们，观念不同再怎么磨合，都是没有用的。因为我无法撬开现实的枷锁，也自然无法领略到你所在的浪漫穷奇的世界。</p><p>“曲终人不见，江上数峰青”。我真的还爱你吗，真的还在想你吗，我没有给自己找借口找理由吗，真的不是已经不喜欢了吗。为什么分手就一定要是朋友都做不下去，为什么分手就要否定当初发生的一切。真的是现在恨的那么深，只因相遇那么美吗？</p><p>我跟朋友开玩笑说，我要当渣n，我再也不会这么认真的去付出自己了，但我知道我肯定做不到，这是玷污爱情，也是在玷污自己和那个ta。</p><p>不管你们说我是装深情人设也好，我是渣n也好，我在谈恋爱的时候绝对100%真心付出，我永远至死不渝的尊重爱情二字。</p><p>“当时明月在，曾照彩云归”。往后再见，一定要变得优秀啊，我永远记得那些繁星满天的夜晚。晚安！</p>]]></content>
      
      
      <categories>
          
          <category> 一往而深 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感情 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序</title>
      <link href="/2022/11/13/xu/"/>
      <url>/2022/11/13/xu/</url>
      
        <content type="html"><![CDATA[<p>似乎早就该写点什么，但因为深陷于生活的囹圄当中，唯一不缺的就是推脱各项事务的借口。好友C前不久放弃复习时间建立网站，这种“敌军围困万千重，我自岿然不动”的精神令我十分触动，便下定决心写点什么，为这个带有“复习不完了”的悲壮色彩建立的网站添砖加瓦。</p><p>通过巧妙地排列组合文字从而精准地传达自己的情感实在是一种令人艳羡的天赋，但相比这种不可强求的能力，我坚信文章最宝贵的品质应当是易读。简单来说就是要说人话，在此基础上应当朝着有趣、简练、严谨努力，至于情感上的共鸣则完全是可遇而不可求了。我自认为在易读性方面做得尚可，但文字的信息熵总是维持在一个极低的水平，我写作时总是难免陷入碎碎念当中不可自拔，一来是我本身就不擅长于对于信息的取舍和整理，每每作答主观题之时都难免洋洋洒洒写满答题区域，寄希望于阅卷老师能从中提取出答案。二来是生活在这个连开心都要用一长串的“哈”才能体现的时代，词义弱化现象的加剧无疑加重了我碎碎念的病情。</p><p>高中时代就曾模仿王小波的风格进行过写作，结果写成的文章叙事风格像王朔一样贫嘴，精神内核像冯唐一样贫瘠，但往好处想起码也是沾了点作家的边，这样看来我似乎也能大言不惭的说一句“小波是好样的，我也是好样的！”</p><p>为摆脱烦闷的高三生活，加上Z同学的所作所为也确实足够戏剧化，我便开始了写作，我将书取名为《肺炎时期的爱情》，结合时事又致敬经典，当时我自诩为严肃作家，自然是瞧不上朱男这样的轻小说作家，更因为我的作品源于生活，他的作品全是幻想。随着高考的临近，曾经的写作热情也不复存在，一切是那么自然而又合乎情理，合理到令人现在回想不免感到沮丧。现在大学的生活远比高三烦闷无聊得多，但却再也找不回那种想要创作一部惊世骇俗著作的冲动，当你习惯了生活是沉闷和孤单后就很难去摆脱它了。</p><p>正如叔本华所言，人生不过是一座钟摆，在痛苦和无聊中摆动。也不知道是幸运亦或者是不幸，这段时间我并不无聊，在无数个辗转反侧的深夜，我都会拿起聂鲁达的诗集，在温柔的文字背后感受他的形象，一位兼具英雄主义和浪漫主义的革命家和诗人，他的一生就像无数革命者一样，充满了传奇色彩却又草草落幕。</p><p>好友T曾开玩笑说我说不定能成为俄国作家，因为痛苦的人都成了俄国作家，对此我不敢苟同，不是因为觉得俄国作家不痛苦，而是因为我想写点历史，如果这种说法是真的，那我为了成功地创作历史专著可能不得不去步太史公的后尘，这是我万万不想见到的。这也是我的语文老师刘煜老师所不愿见到的，他曾多次痛心疾首地要求我们多积累作文素材，不要老盯着人家司马迁那点儿事一直写。说来惭愧，当时写下的张岱的素材一直没能用上，不过司马迁倒是被用到了现在。</p><p>张岱用佛家的果报之说消释了命运剧变当中的种种不可解，所以才能冷冷嘲讽自己“大梦将寤，犹事雕虫”的痴傻，他最终选择用历历如画笔回忆那梦幻般的日子，并在回忆当中找到了心的归属，既然不愿效仿太史公，那还是努力接近陶庵的心境罢。</p><p>是为序。</p>]]></content>
      
      
      <categories>
          
          <category> 肺炎时期的爱情 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序言 </tag>
            
            <tag> 感情 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开始的开始</title>
      <link href="/2022/11/06/kai-shi-de-kai-shi/"/>
      <url>/2022/11/06/kai-shi-de-kai-shi/</url>
      
        <content type="html"><![CDATA[<center>“其实，人生在世，是不太需要别人的建议的，不经历过不会明白的。“</center><p align="right">--Deft</p><p><img src="https://cdn.jsdelivr.net/gh/waitNightWalk/ty_Pic/imgs/image-20221106230623580.png" alt="Deft"></p><p>近些日子，总想搭建一个自己的网站，一来用于消遣，因为大学生活实在无趣，二来偷偷写些矫揉造作的文字，又不愿无人知晓，便将其发布于该网站上，等一些好心人，三来人生确乎需要一些记载，所谓今生今世的证据。</p><p>原计划熬过最近的两门考试在折腾，然而今日的S12总决赛实在过于精彩，盘算着边看边搞，比赛结束打了两局游戏到大概三点，一忙便到夜里，至于考试，我大抵是来不得复习的。原因恐怕是大脑不在状态，产生了两个极小极简单的bug，令人感叹，大学或将我的记忆力与思辨力悄然抹去。</p><p>戴先生最终拿了冠军，可喜可贺，非常羡慕能为某一件热爱的事业专注的人。</p><p>总而言之，本站用于记录一些生活随想和我折腾的经历，并会写一写对过往人生的回忆，不只是我个人的，还会邀请三五好友充实网站。</p><p>无情岁月有味诗，愿大家都能不为遗憾桎梏，活在这珍贵的人间。</p>]]></content>
      
      
      <categories>
          
          <category> 无情岁月有味诗 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 初衷 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
